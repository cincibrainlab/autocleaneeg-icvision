---
title: "RFC-001: Strip Layout Integration for ICVision"
subtitle: "Drop-in Replacement Proposal for autocleaneeg-pipeline"
date: "2026-01-15"
author: "ICVision Development Team"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    mermaid:
      theme: default
---

## Abstract

This document describes the current single-image implementation of ICVision and proposes a plan to integrate the strip layout as a drop-in replacement. The strip layout combines multiple ICA components into a single image for batch classification, reducing API calls from N to N/9 while maintaining full compatibility with the autocleaneeg-pipeline output format.

## Status

| Field | Value |
|-------|-------|
| Status | **Phase 4 Complete + Bug Fixes** |
| Created | 2026-01-15 |
| Updated | 2026-01-15 |
| Target | v0.3.0 |
| Dependencies | test_grid_classify.py (experimental) |
| Model | gpt-5.2 (via custom endpoint) |
| Tests | 58/61 passing (3 pre-existing failures) |
| Bug Fixes | PDF report panels, PSD cutoff (45Hz) |

## Current Single-Image Implementation

### Architecture Overview

The current ICVision implementation processes each ICA component individually. For each component, it generates a single WebP image containing four diagnostic plots, sends that image to the vision API, and receives a classification response. This approach is simple and reliable but requires one API call per component.

```{mermaid}
flowchart TD
    A[label_components] --> B[classify_components_batch]
    B --> C[plot_components_batch]
    C --> D{For each component}
    D --> E[plot_component_for_classification]
    E --> F[Save individual WebP]
    F --> G[classify_component_image_openai]
    G --> H[API call per component]
    H --> I{More components?}
    I -->|yes| D
    I -->|no| J[Aggregate results to DataFrame]
    J --> K[Update ICA object]
    K --> L[Save outputs]
```

### Output Files

The single-image implementation produces the following outputs, all of which must be preserved for pipeline compatibility:

| Output File | Pattern | Description |
|-------------|---------|-------------|
| Results CSV | `{basename}_icvis_results.csv` | Per-component classifications |
| Cleaned Raw | `{basename}_icvis_cleaned_raw.fif` | Artifact-removed EEG data |
| Updated ICA | `{basename}_icvis_classified_ica.fif` | ICA with labels and exclusions |
| PDF Report | `{basename}_icvis_report.pdf` | Visual classification report |
| Summary | `{basename}_icvis_summary.txt` | Cost and classification summary |

### DataFrame Schema

The results DataFrame contains these columns, which downstream pipeline stages depend on:

| Column | Type | Description |
|--------|------|-------------|
| `component_index` | int | ICA component number |
| `component_name` | str | Human-readable name (IC0, IC1, ...) |
| `label` | str | Classification (brain, eye, muscle, ...) |
| `mne_label` | str | MNE-compatible label (eog, ecg, ...) |
| `confidence` | float | Model confidence (0.0-1.0) |
| `reason` | str | Classification rationale |
| `exclude_vision` | bool | Whether to exclude this component |

## Strip Layout Implementation (Experimental)

### Current State

The strip layout exists in `test_grid_classify.py` as an experimental feature. It arranges up to 9 components in horizontal rows within a single image, enabling batch classification with one API call.

```{mermaid}
flowchart TD
    A[main] --> B[Load Raw + ICA]
    B --> C[create_grid_image<br/>layout='strip']
    C --> D[Create figure<br/>16 x 2n inches]
    D --> E[GridSpec: n rows x 4 cols]
    E --> F{For each component}
    F --> G[plot_single_component_subplot]
    G --> H{More components?}
    H -->|yes| F
    H -->|no| I[Save single WebP]
    I --> J[classify_grid]
    J --> K[Single API call]
    K --> L[Parse batch JSON response]
```

### Strip Image Structure

Each row in the strip image contains the same four diagnostic plots as the single-image approach, maintaining visual parity:

```{mermaid}
flowchart LR
    subgraph "Strip Layout (9 components = 1 image)"
        direction TB
        subgraph "A: IC0"
            A1[Topo] --- A2[TimeSeries] --- A3[ERP] --- A4[PSD]
        end
        subgraph "B: IC1"
            B1[Topo] --- B2[TimeSeries] --- B3[ERP] --- B4[PSD]
        end
        subgraph "..."
            X1[...] --- X2[...] --- X3[...] --- X4[...]
        end
        subgraph "I: IC8"
            I1[Topo] --- I2[TimeSeries] --- I3[ERP] --- I4[PSD]
        end
    end
```

## Feature Comparison Table

The following table maps every feature of the current single-image implementation to its strip layout equivalent, identifying gaps that must be addressed for drop-in compatibility:

| Feature | Single-Image (Current) | Strip Layout (Proposed) | Status |
|---------|------------------------|-------------------------|--------|
| **Input Handling** | | | |
| Load Raw from .fif | `load_raw_data()` | Reuse existing | Done |
| Load Raw from .set | `load_raw_data()` | Reuse existing | Done |
| Load ICA from .fif | `load_ica_data()` | Reuse existing | Done |
| Auto-detect ICA in .set | `check_eeglab_ica_availability()` | Reuse existing | Done |
| API key validation | `validate_api_key()` | Reuse existing | Done |
| Custom base_url | `--base-url` parameter | Reuse existing | Done |
| **Plotting** | | | |
| Topography map | `ica.plot_components()` | `plot_single_component_subplot()` | Done |
| Time series (2.5s) | Custom matplotlib | Custom matplotlib | Done |
| ERP image | `mne.viz.plot_epochs_image` style | Custom implementation | Done |
| Power spectrum | `psd_array_welch()` | `psd_array_welch()` | Done |
| Image format | WebP | WebP | Done |
| DPI setting | 150 | 150 | Done |
| **Classification** | | | |
| API endpoint | Responses API | Responses API | Done |
| Model selection | `--model` parameter | `--model` parameter | Done |
| Custom prompt | `--prompt-file` | Needs adaptation | **TODO** |
| Confidence extraction | JSON parsing | JSON parsing | Done |
| Label validation | Against COMPONENT_LABELS | Needs integration | **TODO** |
| **Output Generation** | | | |
| Results CSV | `save_results()` | Needs integration | **TODO** |
| Cleaned Raw .fif | `save_cleaned_raw_data()` | Needs integration | **TODO** |
| Cleaned Raw .set | Format preservation | Needs integration | **TODO** |
| Updated ICA .fif | `save_ica_data()` | Needs integration | **TODO** |
| PDF Report | `generate_classification_report()` | Needs integration | **TODO** |
| Summary text | `format_summary_stats()` | Needs integration | **TODO** |
| **ICA Object Updates** | | | |
| Set `ica.labels_` | `_update_ica_with_classifications()` | Needs integration | **TODO** |
| Set `ica.exclude` | Auto-exclude logic | Needs integration | **TODO** |
| MNE label mapping | `ICVISION_TO_MNE_LABEL_MAP` | Needs integration | **TODO** |
| **Pipeline Integration** | | | |
| CLI interface | `autoclean-icvision` command | Needs `--layout` flag | **TODO** |
| Python API | `label_components()` | Needs `layout` parameter | **TODO** |
| ICLabel compat | `icvision.compat` module | Needs integration | **TODO** |
| Cost tracking | Token counting + pricing | Needs adaptation | **TODO** |

## Implementation Plan

### Phase 1: Core Integration

Refactor the strip layout code from `test_grid_classify.py` into the main `icvision` package. This involves:

1. Move `create_grid_image()` to `plotting.py` with layout parameter
2. Move `plot_single_component_subplot()` to `plotting.py`
3. Add `classify_components_strip()` to `api.py` for batch classification
4. Ensure precomputed sources optimization applies to strip layout

::: {.callout-note}
## Decision: Strip Size Fixed at 9

**Resolved**: Strip size will be fixed at 9 components per image. This simplifies implementation and matches the tested configuration from `test_grid_classify.py`.

**Remainder Handling**: When total components are not divisible by 9, the final batch contains fewer components. For example, with 127 components:

- Batches 1-14: 9 components each (126 total)
- Batch 15: 1 component (IC126)

The final partial strip is rendered with the same 4-column layout but fewer rows. The API prompt explicitly states the number of components in each strip, so the model correctly identifies all components regardless of strip size.
:::

### Phase 1: Detailed Execution Plan

The following steps describe the exact implementation approach for integrating strip layout into the main package.

#### Step 1.1: Extract Plotting Functions

**Action**: Copy `plot_single_component_subplot()` from `test_grid_classify.py` to `src/icvision/plotting.py`.

**Current location**: `test_grid_classify.py` lines ~50-150

**Target signature**:
```python
def plot_single_component_subplot(
    fig: plt.Figure,
    axes: List[plt.Axes],
    ica_obj: mne.preprocessing.ICA,
    raw_obj: mne.io.Raw,
    comp_idx: int,
    precomputed_sources: Optional[mne.io.Raw] = None,
    psd_fmax: float = 45.0,
) -> None:
```

**Pitfalls**:

- The experimental code may not use precomputed sources consistently. Must verify the optimization is applied.
- Axis indexing differs between single-image (4 axes) and strip (4 × n axes). Ensure correct subplot targeting.
- Color scales for topography maps must remain consistent across components in the same strip.

#### Step 1.2: Extract Grid Image Creation

**Action**: Copy `create_grid_image()` from `test_grid_classify.py` to `src/icvision/plotting.py`.

**Target signature**:
```python
def create_strip_image(
    ica_obj: mne.preprocessing.ICA,
    raw_obj: mne.io.Raw,
    component_indices: List[int],
    output_path: Path,
    precomputed_sources: Optional[mne.io.Raw] = None,
    psd_fmax: float = 45.0,
    dpi: int = 150,
) -> Path:
```

**Pitfalls**:

- Figure sizing: current implementation uses `16 x (2 * n_components)` inches. With 9 components, this is 16×18 inches at 150 DPI = 2400×2700 pixels. Verify this stays within API image size limits.
- Memory usage: large figures consume significant RAM. Must call `plt.close(fig)` after saving.
- GridSpec configuration must match the 4-column layout exactly.

#### Step 1.3: Add Strip Classification Function

**Action**: Create `classify_strip_image()` in `src/icvision/api.py`.

**Target signature**:
```python
def classify_strip_image(
    image_path: Path,
    component_indices: List[int],
    api_key: str,
    model_name: str = "gpt-5.2",
    custom_prompt: Optional[str] = None,
    base_url: Optional[str] = None,
) -> List[Dict[str, Any]]:
```

**Pitfalls**:

- **Prompt engineering**: The prompt must clearly specify component ordering (top-to-bottom) and expected JSON array format. Ambiguity leads to misaligned labels.
- **JSON parsing**: Response may include markdown code fences (```json). Must strip these before parsing.
- **Component count mismatch**: Model may return fewer or more classifications than components in the strip. Must validate array length matches `len(component_indices)`.

#### Step 1.4: Create Batch Orchestration

**Action**: Create `classify_components_strip_batch()` in `src/icvision/api.py` to handle windowing.

```python
def classify_components_strip_batch(
    ica_obj: mne.preprocessing.ICA,
    raw_obj: mne.io.Raw,
    api_key: str,
    model_name: str = "gpt-5.2",
    output_dir: Path,
    component_indices: Optional[List[int]] = None,
    psd_fmax: Optional[float] = None,
    base_url: Optional[str] = None,
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
```

**Windowing logic**:
```python
STRIP_SIZE = 9
all_indices = component_indices or list(range(ica_obj.n_components_))
batches = [all_indices[i:i+STRIP_SIZE] for i in range(0, len(all_indices), STRIP_SIZE)]

# batches for 127 components:
# [[0-8], [9-17], [18-26], ..., [117-125], [126]]
```

**Pitfalls**:

- **Partial batches**: The final batch may have 1-8 components. The strip image and prompt must handle this gracefully.
- **Progress tracking**: With 15 batches for 127 components, users need progress feedback. Add logging at each batch completion.
- **Cost aggregation**: Must sum token usage across all batches for accurate cost reporting.

#### Step 1.5: Integration Point

**Action**: Modify `classify_components_batch()` in `src/icvision/api.py` to dispatch based on layout parameter.

```python
def classify_components_batch(
    ...,
    layout: str = "single",  # NEW
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    if layout == "strip":
        return classify_components_strip_batch(...)
    else:
        return _classify_components_single_batch(...)  # existing logic
```

**Pitfalls**:

- **Parameter passthrough**: All existing parameters (confidence_threshold, auto_exclude, labels_to_exclude) must flow through to the strip path.
- **DataFrame schema parity**: Strip results must produce identical column names and types as single-image results.

```{mermaid}
flowchart TD
    subgraph "Phase 1 Execution Flow"
        A[Start] --> B[Step 1.1: Extract plot_single_component_subplot]
        B --> C[Step 1.2: Extract create_strip_image]
        C --> D[Step 1.3: Add classify_strip_image]
        D --> E[Step 1.4: Create batch orchestration]
        E --> F[Step 1.5: Integration dispatch]
        F --> G[Unit tests for each step]
        G --> H[Integration test: 127 components]
        H --> I{Tests pass?}
        I -->|yes| J[Phase 1 Complete]
        I -->|no| K[Debug and iterate]
        K --> G
    end
```

### Phase 1: Completion Status

Phase 1 implementation is complete. The following table documents what was delivered:

| Step | Task | Location | Status | Notes |
|------|------|----------|--------|-------|
| 1.1 | Extract `plot_single_component_subplot()` | `plotting.py:530` | ✅ Complete | Plots single ICA component into provided axes dict (topo, ts, erp, psd) |
| 1.2 | Extract `create_strip_image()` | `plotting.py:702` | ✅ Complete | Creates 4-column strip image with N components (1-52 supported) |
| 1.3 | Add `classify_strip_image()` | `api.py:289` | ✅ Complete | Sends strip to API, parses JSON array, maps letter labels to indices |
| 1.4 | Create batch orchestration | `api.py:461` | ✅ Complete | `classify_components_strip_batch()` with windowing logic |
| 1.5 | Integration dispatch | `api.py` | ✅ Complete | `layout` and `strip_size` parameters added to `classify_components_batch()` |

**Key implementation details**:

- Precomputed ICA sources optimization is applied to avoid redundant computation
- Remainder handling works correctly: final batch accepts 1-8 components with same layout
- Error handling: failed batches fall back to `"other_artifact"` classification
- DataFrame schema parity: strip results produce identical columns as single-image mode

**API surface added**:
```python
classify_components_batch(
    ...,
    layout="strip",      # NEW: "single" or "strip"
    strip_size=9,        # NEW: components per strip (default: 9)
)
```

### Phase 2: Output Compatibility

Ensure strip classification results integrate with existing output functions:

1. Adapt JSON response parsing to produce identical DataFrame schema
2. Verify `save_results()` works with strip-generated DataFrames
3. Verify `_update_ica_with_classifications()` accepts strip results
4. Test all output file generation paths

::: {.callout-note}
## Phase 2 Preflight Check

**Status**: Ready to proceed. Preparation findings:

| Component | Location | Integration Status |
|-----------|----------|-------------------|
| `save_results()` | `utils.py:344` | DataFrame columns match expected schema; no changes required |
| `_update_ica_with_classifications()` | `core.py:310` | Accepts any DataFrame with `component_index`, `label`, `mne_label`, `exclude_vision`; compatible |
| `generate_classification_report()` | `reports.py:164` | Requires individual component images; needs adaptation for strip mode |
| Label validation | `config.py` | `COMPONENT_LABELS` list exists; already integrated in strip parsing |
| Cost tracking | `api.py` | Token usage aggregated across batches; working |

**Ready items** (no changes needed):
- CSV output via `save_results()` — strip DataFrame schema already matches
- ICA object updates via `_update_ica_with_classifications()` — compatible
- Cleaned Raw export — independent of classification method

**Resolved**:
- ✅ PDF report generation: Option A selected (individual images) — works without code changes
- Custom prompt file support (`--prompt-file`) not yet adapted for strip mode (Phase 3)
:::

### Phase 2: Completion Status

Phase 2 (Output Compatibility) is complete. TDD approach was used: tests written first, then code fixed to pass.

| Task | Status | Details |
|------|--------|---------|
| DataFrame schema parity | ✅ Complete | Fixed column names: `component_index`, `component_name`, `label`, `exclude_vision` |
| `save_results()` integration | ✅ Complete | Strip DataFrame accepted without modification |
| `_update_ica_with_classifications()` | ✅ Complete | ICA labels and exclusions correctly set from strip results |
| DataFrame index | ✅ Complete | Index set to `component_index` for downstream compatibility |
| MNE label mapping | ✅ Complete | `mne_label` column correctly maps icvision labels |
| Remainder handling | ✅ Complete | Partial batches (1-8 components) produce valid DataFrame rows |

**Code change** (`api.py:600-613`):

Schema was corrected from:
```python
# OLD (incompatible)
{"component": ..., "ic_type": ..., "exclude": ...}
```

to:
```python
# NEW (compatible)
{"component_index": ..., "component_name": f"IC{idx}", "label": ..., "exclude_vision": ...}
```

**Test suite** (`tests/test_strip_compatibility.py`): 10 tests, all passing.

::: {.callout-note}
## Decision Resolved: PDF Report Content

**Option A selected**: Generate individual images for the report (slower, but familiar format).

This approach maintains visual consistency with single-image mode reports. The `generate_classification_report()` function already generates individual component images fresh from the ICA object, independent of how classification was performed.

**Verification**: 2 tests added to `test_strip_compatibility.py`:
- `test_generate_report_accepts_strip_dataframe` — PDF generated successfully
- `test_generate_report_artifacts_only_with_strip_dataframe` — Artifacts-only mode works

**No code changes required** — existing report generation works with strip DataFrame thanks to Phase 2 schema fix.
:::

### Phase 3: CLI and API Surface

Add layout selection to public interfaces:

1. Add `--layout` flag to CLI (values: `single`, `strip`)
2. Add `layout` parameter to `label_components()`
3. Add `layout` parameter to `icvision.compat.label_components()`
4. Default to `single` for backward compatibility

::: {.callout-note}
## Decision Resolved: Default Behavior

**Default is `single`** for backward compatibility. This can be revisited in a future version if strip mode proves superior.
:::

### Phase 3: Completion Status

Phase 3 (CLI and API Surface) is complete. TDD: 10 tests written, all passing.

| Task | Status | Details |
|------|--------|---------|
| `--layout` CLI flag | ✅ Complete | Accepts `single` or `strip`, default `single` |
| `--strip-size` CLI flag | ✅ Complete | Default 9, configurable |
| `layout` in `label_components()` | ✅ Complete | `core.py` parameter added |
| `layout` in `compat.label_components()` | ✅ Complete | Passthrough to core |
| Parameter passthrough | ✅ Complete | Layout/strip_size flow to `classify_components_batch()` |

**CLI usage**:
```bash
# Single-image mode (default, backward compatible)
autoclean-icvision raw.set

# Strip mode (batch 9 components per image)
autoclean-icvision raw.set --layout strip

# Custom strip size
autoclean-icvision raw.set --layout strip --strip-size 12
```

**Test suite**: `tests/test_phase3_cli_api.py` (10 tests)

### Phase 4: Batch Windowing

Implement sliding window for large component counts:

1. For N > 9 components, process in batches of 9
2. Aggregate results across batches into single DataFrame
3. Handle edge cases (N not divisible by 9)

```{mermaid}
flowchart TD
    A[127 components] --> B{N > 9?}
    B -->|yes| C[Split into batches of 9]
    C --> D[Batch 1: IC0-IC8]
    C --> E[Batch 2: IC9-IC17]
    C --> F[...]
    C --> G[Batch 15: IC126]
    D --> H[Strip classify]
    E --> H
    F --> H
    G --> H
    H --> I[Merge DataFrames]
    I --> J[Single results_df]
    B -->|no| K[Single strip image]
    K --> H
```

::: {.callout-note}
## Decision Resolved: Partial Batch Failures

**Retry failed batch with exponential backoff** selected. Implementation:

- Max 3 retries per batch
- Exponential backoff: 1s, 2s, 4s delays
- On exhausted retries: fall back to `other_artifact` label
:::

### Phase 4: Completion Status

Phase 4 (Error Handling) is complete. TDD: 7 tests written, all passing.

| Task | Status | Details |
|------|--------|---------|
| `max_retries` parameter | ✅ Complete | Added to `classify_strip_image()`, default 3 |
| Exponential backoff | ✅ Complete | Delays: 1s, 2s, 4s between retries |
| Retry on API failure | ✅ Complete | Transient errors trigger retry |
| Fallback on exhausted | ✅ Complete | Failed components labeled `other_artifact` |

**Implementation** (`api.py`):
```python
# Retry loop with exponential backoff
for attempt in range(max_retries):
    try:
        result = _call_openai_api(...)
        break
    except Exception:
        backoff_time = 2 ** attempt  # 1s, 2s, 4s
        time.sleep(backoff_time)
```

**Test suite**: `tests/test_phase4_retry.py` (7 tests)

## API Changes

### Proposed label_components Signature

```python
def label_components(
    raw_data: Union[str, Path, mne.io.Raw],
    ica_data: Optional[Union[str, Path, mne.preprocessing.ICA]] = None,
    api_key: Optional[str] = None,
    confidence_threshold: float = 0.8,
    auto_exclude: bool = True,
    labels_to_exclude: Optional[List[str]] = None,
    output_dir: Optional[Union[str, Path]] = None,
    generate_report: bool = True,
    batch_size: int = 10,
    max_concurrency: int = 4,
    model_name: str = "gpt-5.2",
    custom_prompt: Optional[str] = None,
    component_indices: Optional[List[int]] = None,
    psd_fmax: Optional[float] = None,
    base_url: Optional[str] = None,
    layout: str = "single",  # NEW: "single" or "strip"
    strip_size: int = 9,     # NEW: components per strip image
) -> Tuple[mne.io.Raw, mne.preprocessing.ICA, pd.DataFrame]:
```

### Proposed CLI Addition

```bash
autoclean-icvision raw.set \
  --layout strip \
  --strip-size 9 \
  --model gpt-5.2 \
  --base-url https://openai.cincibrainlab.com/v1
```

::: {.callout-tip}
## Suggestion: Environment Variable Override

For pipeline automation, consider supporting `ICVISION_LAYOUT` environment variable so operators can switch modes without modifying scripts:

```bash
export ICVISION_LAYOUT=strip
autoclean-icvision raw.set  # Uses strip mode
```
:::

## Performance Projections

| Metric | Single-Image | Strip (9) | Improvement |
|--------|--------------|-----------|-------------|
| API calls (127 comp) | 127 | 15 | 88% reduction |
| Latency per component | ~3.5s | ~0.4s | 89% reduction |
| Total time (127 comp) | ~445s | ~53s | 88% reduction |
| Token caching | Per component | Per strip | Higher hit rate |

::: {.callout-warning}
## Validation Required: Accuracy Comparison

The performance gains assume strip classification maintains accuracy parity with single-image mode. The experimental results in `experiments/grid_tests/results.md` show promising results, but we need:

1. Larger sample size validation (>10 subjects)
2. Per-category accuracy breakdown (brain, eye, muscle, etc.)
3. Expert review of disagreement cases

Do you have additional test data for validation?
:::

## Open Questions Summary

::: {.callout-important}
## Questions Requiring Your Input

1. ~~**Strip size**: Fixed at 9, or configurable?~~ **RESOLVED: Fixed at 9**
2. ~~**PDF report**: Individual images, strip images, or skip?~~ **RESOLVED: Option A (individual images)**
3. ~~**Default behavior**: Keep `single` forever, or plan transition?~~ **RESOLVED: `single` default**
4. ~~**Error handling**: Fail, retry, fallback, or skip?~~ **RESOLVED: Retry with exponential backoff**
5. **Validation data**: Available test subjects for accuracy comparison?
:::

## Pipeline Integration: autocleaneeg_pipeline

### Original Implementation

ICVision is integrated into the `autocleaneeg_pipeline` as an alternative ICA classification method alongside ICLabel. The integration lives in `src/autoclean/functions/ica/ica_processing.py` and uses the `icvision.compat.label_components()` compatibility layer to maintain API parity with MNE-ICLabel.

The pipeline supports three classification modes:

| Mode | Description | Use Case |
|------|-------------|----------|
| `iclabel` | MNE-ICLabel only | Fast local classification, no API cost |
| `icvision` | ICVision only | Vision-based classification for all components |
| `hybrid` | ICLabel first, ICVision for top N | Balance cost and accuracy |

The **hybrid mode** is particularly interesting: it runs ICLabel on all components first, then reclassifies the first N components (default: 20) using ICVision. This approach assumes that the most important components to accurately classify are the highest-variance ones (lowest indices), while allowing cheaper ICLabel classifications for the tail components that are often noise.

### Integration Flow

```{mermaid}
flowchart TD
    subgraph "autocleaneeg_pipeline"
        A[ica_processing.classify_ica_components] --> B{method?}
        B -->|iclabel| C[mne_icalabel.label_components]
        B -->|icvision| D[icvision.compat.label_components]
        B -->|hybrid| E[ICLabel all + ICVision subset]
        C --> F[_icalabel_to_dataframe]
        D --> F
        E --> F
        F --> G[_attach_source_metadata]
        G --> H[DataFrame with iclabel/icvision columns]
    end
    subgraph "icvision package"
        D --> I[core.label_components]
        I --> J[api.classify_components_batch]
        J --> K[N individual API calls]
    end
```

The pipeline merges classification results from both sources, preserving:

- `iclabel_ic_type` / `iclabel_confidence` — ICLabel predictions
- `icvision_ic_type` / `icvision_confidence` — ICVision predictions
- `ic_type` / `confidence` — Final merged values
- `annotator` — Source of the final classification

### Current Bottleneck

In hybrid mode with N=20 components and 127 total ICA components, the current implementation makes **20 individual API calls** for the ICVision portion. Each call:

1. Generates a single-component image (topo, time series, ERP, PSD)
2. Encodes image as base64
3. Sends HTTP request to OpenAI Vision API
4. Waits for response
5. Parses JSON classification

With typical API latency of ~3.5 seconds per call, the ICVision portion takes approximately **70 seconds** per dataset.

### Proposed Strip Optimization

The strip layout reduces hybrid mode from 20 API calls to **3 API calls** (ceil(20/9) = 3 batches):

| Batch | Components | Image |
|-------|------------|-------|
| 1 | IC0–IC8 | 9-row strip |
| 2 | IC9–IC17 | 9-row strip |
| 3 | IC18–IC19 | 2-row strip |

```{mermaid}
flowchart TD
    subgraph "Proposed: Strip Mode in Hybrid"
        A[classify_ica_components method=hybrid] --> B[ICLabel all components]
        B --> C[icvision.compat.label_components<br/>layout='strip', component_indices=0..19]
        C --> D[create_strip_image batch 1<br/>IC0-IC8]
        C --> E[create_strip_image batch 2<br/>IC9-IC17]
        C --> F[create_strip_image batch 3<br/>IC18-IC19]
        D --> G[Single API call]
        E --> G
        F --> G
        G --> H[Parse 3 JSON responses]
        H --> I[Merge with ICLabel results]
    end
```

### Cost and Time Savings

The following table projects savings for a typical hybrid workflow (20 ICVision components):

| Metric | Single-Image | Strip (9) | Reduction |
|--------|--------------|-----------|-----------|
| API calls | 20 | 3 | 85% |
| Total latency | ~70s | ~11s | 84% |
| Token cost | 20 × prompt + 20 × response | 3 × prompt + 3 × response | ~85% |
| Image generation time | 20 × 0.3s = 6s | 3 × 0.5s = 1.5s | 75% |

For full ICVision mode (127 components):

| Metric | Single-Image | Strip (9) | Reduction |
|--------|--------------|-----------|-----------|
| API calls | 127 | 15 | 88% |
| Total latency | ~445s | ~53s | 88% |
| Token cost | 127 × base | 15 × base | ~88% |

### Implementation Plan for Pipeline Integration

The strip layout is already implemented in ICVision (Phases 1-4 complete). The following steps enable pipeline usage:

| Step | Task | Location | Status |
|------|------|----------|--------|
| 1 | Add `layout` parameter to `compat.label_components()` | `icvision/compat.py` | ✅ Complete |
| 2 | Pass `layout` through to `core.label_components()` | `icvision/compat.py` | ✅ Complete |
| 3 | Dispatch to strip batch in `classify_components_batch()` | `icvision/api.py` | ✅ Complete |
| 4 | Update pipeline kwargs to pass `layout='strip'` | `ica_processing.py` | ✅ Complete |
| 5 | Test hybrid mode with strip layout | Integration test | **TODO** |
| 6 | Measure accuracy vs single-image mode | Validation study | **TODO** |

### Backward Compatibility

The integration is designed for zero-disruption adoption:

1. **Default behavior unchanged**: `layout="single"` is default, existing pipelines work identically
2. **Opt-in activation**: Pipeline must explicitly pass `layout="strip"` to enable
3. **DataFrame schema preserved**: Strip results produce identical columns
4. **Fallback preserved**: If ICVision fails, ICLabel results are used (existing behavior)

To enable strip mode in the pipeline, modify the `classify_ica_components()` call:

```python
# Before (single-image mode)
label_components(raw, ica, component_indices=component_indices, **kwargs)

# After (strip mode)
label_components(raw, ica, component_indices=component_indices, layout="strip", **kwargs)
```

Or pass through kwargs from the pipeline configuration:

```python
# In ica_processing.py classify_ica_components()
icvision_kwargs = {"layout": "strip", **kwargs}  # Allow user override
label_components(raw, ica, **icvision_kwargs)
```

::: {.callout-note}
## Step 4 Complete: Pipeline Now Uses Strip Mode by Default

The `autocleaneeg_pipeline` now defaults to strip layout for ICVision classification. Changes made to `ica_processing.py`:

1. **icvision method** (line 212): Added `icvision_kwargs = {"layout": "strip", **kwargs}`
2. **hybrid method** (line 275): Added `icvision_kwargs = {"layout": "strip", **kwargs}`
3. **Docstring updated**: Documents `layout` parameter with default 'strip'

Users can still override with `layout="single"` if needed. The change is backward compatible.
:::

### Running the Pipeline with Strip Mode

With the pipeline integration complete, strip mode is now the default for all ICVision classification. Here's how to run and verify the pipeline:

**Prerequisites**:

1. **Workspace configured**: The workspace directory (e.g., `~/sandbox/Autoclean-EEG`) must contain:
   - `.env` file with API credentials:
     ```
     OPENAI_BASE_URL=https://openai.cincibrainlab.com/v1
     OPENAI_API_KEY=<your-api-key>
     ```
   - `tasks/` directory with task configuration files

2. **Task file**: Task must use `hybrid` or `icvision` method with the gpt-5.2 model:
   ```python
   'component_rejection': {
       'enabled': True,
       'method': 'hybrid',           # or 'icvision'
       'model_name': 'gpt-5.2',
       'value': {
           'ic_flags_to_reject': ['muscle', 'heart', 'eog', 'ch_noise', 'line_noise'],
           'ic_rejection_threshold': 0.3,
           'psd_fmax': 45             # Recommended: avoid notch filter artifacts
       }
   }
   ```

**Command-line execution**:

```bash
# Basic usage (uses active task and input from workspace settings)
autocleaneeg-pipeline process

# Explicit task file and input
autocleaneeg-pipeline process \
    --task-file /path/to/tasks/BiotrialResting1020.py \
    --file /path/to/201001_D1BL_EC.set \
    --output /path/to/output

# Dry run (shows what would be processed without running)
autocleaneeg-pipeline process --dry-run

# Non-interactive mode (for automation)
autocleaneeg-pipeline process --yes
```

**Example with test data**:

```bash
# Using the BiotrialResting1020 task with test EEG file
autocleaneeg-pipeline process \
    --task-file ~/sandbox/Autoclean-EEG/tasks/BiotrialResting1020.py \
    --file ~/Downloads/qEEG/201001_D1BL_EC.set \
    --output ~/sandbox/Autoclean-EEG/output
```

**Verifying strip mode is active**:

In the pipeline logs, you should see:
- `"layout": "strip"` in ICVision parameters
- Batch processing messages like "Processing strip batch 1/N"
- Reduced API call count (N/9 instead of N calls for N components)

::: {.callout-tip}
## Overriding Layout Mode

To revert to single-image mode for debugging or comparison, pass `layout='single'` in the task configuration:

```python
'component_rejection': {
    'enabled': True,
    'method': 'hybrid',
    'value': {
        'layout': 'single',  # Override: use single-image mode
        # ... other settings
    }
}
```
:::

## Benchmark: Single vs Strip Mode Classification

Real-world benchmark using test data: `201001_D1BL_EC.set` (24 ICA components)

### Performance Comparison

| Metric | Single Mode | Strip Mode | Improvement |
|--------|-------------|------------|-------------|
| **Total Time** | 66.50s | 51.24s | **23% faster** |
| **API Calls** | 24 | 3 | **87.5% reduction** |
| **Estimated Cost** | $0.29 | ~$0.04 | **~86% savings** |
| **Per-component** | 2.77s | 2.14s | 23% faster |

### Classification Agreement

| Component | Single Label | Conf | Strip Label | Conf | Match |
|-----------|--------------|------|-------------|------|-------|
| IC0 | eye | 0.78 | eye | 0.85 | ✓ |
| IC1 | muscle | 0.86 | muscle | 0.75 | ✓ |
| IC2 | brain | 0.87 | brain | 0.90 | ✓ |
| IC3 | brain | 0.78 | brain | 0.70 | ✓ |
| IC4 | brain | 0.82 | brain | 0.65 | ✓ |
| IC5 | brain | 0.72 | muscle | 0.80 | ✗ |
| IC6 | muscle | 0.78 | muscle | 0.85 | ✓ |
| IC7 | brain | 0.78 | brain | 0.75 | ✓ |
| IC8 | muscle | 0.84 | muscle | 0.70 | ✓ |
| IC9 | muscle | 0.82 | muscle | 0.85 | ✓ |
| IC10 | eye | 0.78 | brain | 0.75 | ✗ |
| IC11 | muscle | 0.87 | muscle | 0.80 | ✓ |
| IC12 | muscle | 0.73 | brain | 0.70 | ✗ |
| IC13 | brain | 0.70 | brain | 0.75 | ✓ |
| IC14 | brain | 0.77 | brain | 0.90 | ✓ |
| IC15 | brain | 0.78 | brain | 0.90 | ✓ |
| IC16 | brain | 0.67 | brain | 0.85 | ✓ |
| IC17 | eye | 0.77 | brain | 0.70 | ✗ |
| IC18 | brain | 0.67 | brain | 0.72 | ✓ |
| IC19 | brain | 0.74 | brain | 0.70 | ✓ |
| IC20 | brain | 0.72 | brain | 0.68 | ✓ |
| IC21 | eye | 0.78 | channel_noise | 0.78 | ✗ |
| IC22 | brain | 0.66 | brain | 0.74 | ✓ |
| IC23 | brain | 0.72 | brain | 0.66 | ✓ |

**Agreement: 19/24 (79.2%)**

::: {.callout-note}
## Classification Differences Analysis

5 components showed different labels between modes:

- **IC5**: Single=brain, Strip=muscle (edge case, low confidence both modes)
- **IC10**: Single=eye, Strip=brain (frontal component, ambiguous)
- **IC12**: Single=muscle, Strip=brain (borderline spectral features)
- **IC17**: Single=eye, Strip=brain (frontal/periocular uncertainty)
- **IC21**: Single=eye, Strip=channel_noise (focal pattern interpretation)

These disagreements occur in components with:
1. Ambiguous topographies (frontal/edge patterns)
2. Lower confidence scores (typically <0.80)
3. Borderline spectral features

Both modes correctly identify clear artifacts (muscle IC1, IC6, IC9, IC11) and clear brain components (IC2, IC14, IC15).
:::

### Exclusion Summary

| Mode | Components Excluded | Labels |
|------|---------------------|--------|
| Single | 4 | IC1 (muscle), IC8 (muscle), IC9 (muscle), IC11 (muscle) |
| Strip | 5 | IC0 (eye), IC5 (muscle), IC6 (muscle), IC9 (muscle), IC11 (muscle) |

### Strip Mode Classification Images

The following images show the actual strip batches sent to the API for classification. Each strip contains up to 9 components arranged in rows, with 4 columns: Topography, Time Series, ERP, and PSD (1-45Hz).

**Batch 1: Components IC0-IC8**

![Strip Batch 0: Components IC0-IC8](images/strip_batch_0.png){width=100%}

**Batch 2: Components IC9-IC17**

![Strip Batch 1: Components IC9-IC17](images/strip_batch_1.png){width=100%}

**Batch 3: Components IC18-IC23 (6 components)**

![Strip Batch 2: Components IC18-IC23](images/strip_batch_2.png){width=100%}

::: {.callout-tip}
## Visual Review Guide

When reviewing these strips, look for:

1. **Eye artifacts**: Strong frontal topography, low-frequency dominated PSD, large slow deflections in time series
2. **Muscle artifacts**: Edge/temporal focal topography, high-frequency dominated PSD (rising toward 45Hz)
3. **Brain components**: Dipolar topography, clear alpha peak (8-12Hz) in PSD, 1/f spectral falloff
4. **Channel noise**: Single focal spot in topography, flat/noisy spectrum
:::

### API Call Details

The strip classification uses OpenAI's Responses API with the following structure:

**Endpoint**: `client.responses.create()`

**Parameters**:
```python
response = client.responses.create(
    model="gpt-5.2",
    input=[
        {"role": "user", "content": <PROMPT>},
        {
            "role": "user",
            "content": [
                {
                    "type": "input_image",
                    "image_url": "data:image/webp;base64,<BASE64_IMAGE>"
                }
            ]
        }
    ],
    temperature=0.2
)
```

**Classification Prompt** (for 9 components):

```
Classify each of the 9 ICA components shown in this grid (labeled A, B, C, D, E, F, G, H, I).

Each component shows:
- Topography map (scalp distribution)
- Time series (first 2.5 seconds)
- ERP-style image (continuous data segments)
- Power spectrum (1-55Hz)

Categories:
- "brain": Dipolar pattern (can be central, parietal, OR lateral/temporal),
  1/f spectrum with alpha (8-12Hz) or beta (13-30Hz) peaks
- "eye": Frontal/periocular focus with low-frequency dominated spectrum (<4Hz)
  AND large slow deflections in time series
- "muscle": Edge-focused topography AND flat/rising high-frequency spectrum (no alpha peak)
- "heart": ~1Hz rhythmic deflections in time series, broad scalp distribution
- "line_noise": Sharp narrow peak at 50/60Hz
- "channel_noise": Single isolated focal spot (one sensor) with flat/noisy spectrum
- "other_artifact": Doesn't fit above categories

Respond with JSON array (one object per component):
[
  {"component": "A", "label": "category", "confidence": 0.0-1.0, "reason": "..."},
  {"component": "B", "label": "category", "confidence": 0.0-1.0, "reason": "..."},
  ...
]
```

::: {.callout-note}
## Reasoning Mode for GPT-5.2

The API call uses **default reasoning behavior** with these characteristics:

1. **API Type**: OpenAI Responses API (not Chat Completions)
2. **No explicit `reasoning_effort` parameter** is set
3. **Temperature**: 0.2 (low for consistent, deterministic outputs)
4. **Response format**: JSON array with classification + confidence + reasoning for each component

The model performs visual analysis on the strip image and returns structured JSON with reasoning explanations for each classification decision.
:::

### Low Reasoning Mode Comparison

Tested the `reasoning_effort='low'` parameter to evaluate potential speed/accuracy tradeoffs.

**Test Configuration**:

- Data: 24 ICA components (same test file)
- Model: gpt-5.2
- Endpoint: CLIProxy

**Performance Comparison**:

| Metric | Default Reasoning | Low Reasoning | Difference |
|--------|------------------|---------------|------------|
| **Total Time** | 51.24s | 81.85s | 60% slower |
| **API Calls** | 3 | 3 | Same |
| **Artifacts Found** | 5 | 7 | 2 more |

::: {.callout-warning}
## Unexpected Result: Low Reasoning is Slower

Counter-intuitively, `reasoning_effort='low'` resulted in **longer** processing times. This may be due to:

1. **Endpoint behavior**: CLIProxy may not optimize for low reasoning the same way OpenAI does
2. **Model characteristics**: GPT-5.2 may have different reasoning overhead patterns
3. **Network variability**: Could be coincidental timing differences

**Recommendation**: Use default reasoning (no explicit `reasoning_effort` parameter) for best performance with this endpoint.
:::

**Classification Comparison (Low Reasoning vs Default)**:

| Component | Default Label | Conf | Low Label | Conf | Match |
|-----------|---------------|------|-----------|------|-------|
| IC0 | eye | 0.85 | eye | 0.90 | ✓ |
| IC1 | muscle | 0.75 | muscle | 0.75 | ✓ |
| IC2 | brain | 0.90 | brain | 0.95 | ✓ |
| IC3 | brain | 0.70 | brain | 0.75 | ✓ |
| IC4 | brain | 0.65 | brain | 0.80 | ✓ |
| IC5 | muscle | 0.80 | brain | 0.60 | ✗ |
| IC6 | muscle | 0.85 | muscle | 0.85 | ✓ |
| IC7 | brain | 0.75 | brain | 0.85 | ✓ |
| IC8 | muscle | 0.70 | muscle | 0.55 | ✓ |
| IC9 | muscle | 0.85 | muscle | 0.80 | ✓ |
| IC10 | brain | 0.75 | brain | 0.75 | ✓ |
| IC11 | muscle | 0.80 | muscle | 0.80 | ✓ |
| IC12 | brain | 0.70 | eye | 0.60 | ✗ |
| IC13 | brain | 0.75 | brain | 0.60 | ✓ |
| IC14 | brain | 0.90 | brain | 0.85 | ✓ |
| IC15 | brain | 0.90 | brain | 0.80 | ✓ |
| IC16 | brain | 0.85 | brain | 0.75 | ✓ |
| IC17 | brain | 0.70 | brain | 0.70 | ✓ |
| IC18 | brain | 0.72 | brain | 0.72 | ✓ |
| IC19 | brain | 0.70 | brain | 0.70 | ✓ |
| IC20 | brain | 0.68 | brain | 0.78 | ✓ |
| IC21 | channel_noise | 0.78 | eye | 0.86 | ✗ |
| IC22 | brain | 0.74 | brain | 0.74 | ✓ |
| IC23 | brain | 0.66 | brain | 0.55 | ✓ |

**Agreement: 21/24 (87.5%)**

Three disagreements: IC5 (muscle→brain), IC12 (brain→eye), IC21 (channel_noise→eye). All on ambiguous components.

## Bug Fixes During Strip Integration

### PDF Report Panel Rendering (Fixed)

::: {.callout-important}
## Issue: Excluded Components Showed Flat Data in PDF Reports

When using strip mode, PDF report panels for excluded components showed:

- Time series: Scale "1e-15" (essentially zero)
- ERP image: Uniform green (no variation)
- PSD: Flat line at -200 dB

Only topographies rendered correctly.
:::

**Root Cause**: The `_apply_artifact_rejection()` function modified the raw data **in-place** via `ica.apply(raw)`. When the PDF report was generated, it received the already-modified raw data where excluded component signals had been zeroed out.

**Solution**: Modified `_apply_artifact_rejection()` to work on a copy:

```python
# Before (in-place modification broke PDF report)
def _apply_artifact_rejection(raw, ica):
    if ica.exclude:
        ica.apply(raw)  # Modifies raw in-place!
    return raw

# After (preserves original for PDF report)
def _apply_artifact_rejection(raw, ica):
    raw_cleaned = raw.copy()  # Make copy first
    if ica.exclude:
        ica.apply(raw_cleaned)  # Apply to copy only
    return raw_cleaned
```

**Commits**: `d9262b6`

### PSD Frequency Limit (Changed)

::: {.callout-note}
## Change: Default PSD Cutoff Reduced from 80Hz to 45Hz

EEG data typically has notch filters applied at 50Hz (Europe/Asia) or 60Hz (Americas) to remove line noise. These filters create visible dips in the power spectrum that can confuse visual interpretation by both humans and the AI classifier.
:::

**Solution**: Changed default `psd_fmax` from 80Hz to 45Hz across all plotting functions:

- `plot_component_for_classification()`: 80Hz → 45Hz
- `plot_single_component_subplot()`: 55Hz → 45Hz
- CLI help text updated to reflect new default

This displays clean spectral content through alpha and beta ranges without notch filter artifacts. Users can still specify higher frequencies via `--psd-fmax` flag if needed.

**Commits**: `316ea41`

### Visual Examples

The following strip image demonstrates the 45Hz PSD cutoff in action. Notice how the PSD plots (rightmost column) show clean spectral content without notch filter artifacts:

![Strip layout with 45Hz PSD cutoff - 9 ICA components showing topography, time series, ERP, and power spectrum](images/strip_example_45hz.png){width=100%}

**Key observations**:

1. **Topography (Column 1)**: Scalp maps show component spatial patterns for identifying artifact sources
2. **Time Series (Column 2)**: Raw component activation over time reveals temporal characteristics
3. **ERP/Average (Column 3)**: Event-related patterns highlight stimulus-locked activity
4. **PSD (Column 4)**: Power spectrum now displays 1-45Hz range, avoiding the 50-60Hz notch filter region

::: {.callout-tip}
## Before vs After

The PSD plots previously showed frequencies up to 80Hz, which included visible notch filter dips at 50-60Hz. These dips created artificial-looking "valleys" in the spectrum that could mislead both human reviewers and AI classifiers. By capping at 45Hz, we show meaningful spectral content (delta through beta bands) without these artifacts.
:::

## References

1. Current implementation: `src/icvision/core.py`
2. Experimental strip: `test_grid_classify.py`
3. Experiment results: `experiments/grid_tests/results.md`
4. Pipeline integration: `autocleaneeg_pipeline/src/autoclean/functions/ica/ica_processing.py`
